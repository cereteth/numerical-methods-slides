% Copyright Luke Olson 2009--2014
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a
% copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.
%
\documentclass[10pt]{beamer}
%\documentclass[handout,10pt]{beamer}
%
\mode<presentation>
{
  \usetheme[secheader]{Boadilla}
  \usefonttheme[onlymath]{serif}
  \setbeamercovered{invisible}
  \usecolortheme{luke}
  %\setbeamercovered{transparent}
  %
}
\mode<handout>
{
  \usetheme[secheader]{Boadilla}
  \usefonttheme[onlymath]{serif}
  \setbeamercovered{invisible}
  \usecolortheme{luke2}
  %\setbeamercovered{transparent}
}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{pxfonts}
\usepackage{eulervm}
\usepackage{listings}
%\usepackage{pgfpages}
%\pgfpagesuselayout{2 on 1}[letterpaper]
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%
%
%
\newcommand{\vb}{{\bf{b}}}
\newcommand{\ve}{{\bf{e}}}
\newcommand{\vg}{{\bf{g}}}
\newcommand{\vp}{{\bf{p}}}
\newcommand{\vr}{{\bf{r}}}
\newcommand{\vu}{{\bf{u}}}
\newcommand{\vx}{{\bf{x}}}
\newcommand{\vz}{{\bf{z}}}
\newcommand{\vA}{{\bf{A}}}
\newcommand{\vU}{{\bf{U}}}
\newcommand{\mO}{{\mathcal{O}}}
\newcommand{\mF}{{\mathcal{F}}}
\definecolor{mygray}{rgb}{0.95,0.95,0.95}
\lstset{
        language=matlab,
        numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt,
        basicstyle=\color{black}\ttfamily\small,
        commentstyle=\color{green}\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        showstringspaces=false,
        backgroundcolor=\color{mygray},
        breaklines,
}
\newcommand{\norm}[1]{{\ensuremath{{\|#1\|}}}}
\newcommand{\matdim}[2]{\ensuremath{#1\times#2}}
\newcommand{\rank}[1]{\ensuremath{\mathrm{rank}(#1)}}
\newcommand{\epsm}{\ensuremath{\varepsilon_m}}
\newcommand{\cmd}[1]{{\normalfont\ttfamily\bfseries#1}}

\author{L. Olson}
\institute[UIUC]
{Department of Computer Science\\
University of Illinois at Urbana-Champaign\\
\vspace{0.5cm}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pgfdeclareimage[height=0.5cm]{university-logo}{./figs/uiuclogo}
\logo{\pgfuseimage{university-logo}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[CS 357]{Lecture 8}
\subtitle{Conditioning, Banded Systems}
\date{September 17, 2009}

\begin{document}
% -------------------------------------------------
\begin{frame}
  \titlepage
\end{frame}
% -------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Goals for today...}

\begin{itemize}
  \item How do we know if GE if will be accurate?
    \begin{itemize}
      \item norms, condition number, theory
    \end{itemize}
  \item Can we reduce cost for special systems
    \begin{itemize}
      \item tridiagonals, banded, etc
    \end{itemize}
\end{itemize}
\vfill
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometric Interpretation of Singularity}

Consider a \matdim{2}{2} system describing two lines that intersect
\begin{align*}
    y &= -2x + 6 \\
    y &= \ \ \frac{1}{2} x + 1
\end{align*}
The matrix form of this equation is
\begin{equation*}
    \left[\negthickspace\begin{array}{cr} 2 & 1 \\ -1/2 & 1 \end{array}\negthickspace\right]
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
    =
    \begin{bmatrix} 6 \\ 1 \end{bmatrix}
\end{equation*}
The equations for two \textbf{parallel} but \textbf{not intersecting} lines are
\begin{equation*}
    \begin{bmatrix} 2 & 1 \\ 2 & 1 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
    =
    \begin{bmatrix} 6 \\ 5 \end{bmatrix}
\end{equation*}
Here the coefficient matrix is singular ($\rank{A}=1$), and
the system is inconsistent

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometric Interpretation of Singularity}

The equations for two \textbf{parallel} and \textbf{coincident} lines are
\begin{equation*}
    \begin{bmatrix} 2 & 1 \\ 2 & 1 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
    =
    \begin{bmatrix} 6 \\ 6 \end{bmatrix}
\end{equation*}

The equations for two \textbf{nearly parallel} lines are
\begin{equation*}
    \begin{bmatrix} 2 & 1 \\ 2+\delta & 1 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
    =
    \begin{bmatrix} 6 \\ 6+\delta \end{bmatrix}
\end{equation*}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometric Interpretation of Singularity}

\begin{center}
    \pgfimage[height=7cm]{./figs/systemTypes}
\end{center}


% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $b$}

Consider the solution of a \matdim{2}{2} system where
\begin{equation*}
    b = \begin{bmatrix}1 \\ 2/3 \end{bmatrix}
\end{equation*}
One expects that the
\emph{exact} solutions to
\begin{equation*}
    Ax = \begin{bmatrix}1 \\ 2/3 \end{bmatrix}
    \ \ \ \ \ \ \ \ \
    \text{and}
    \ \ \ \ \ \ \ \ \
    Ax = \begin{bmatrix}1 \\ 0.6667 \end{bmatrix}
\end{equation*}
will be different.  Should these solutions be a \textbf{lot different}
or a \textbf{little different}?

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Norms}
Vectors:
\begin{align*}
    \|x\|_{p} &= \bigl( |x_1|^p + |x_2|^p + \ldots + |x_n|^p \bigr)^{1/p}\\
    \|x\|_{1} &= |x_1| + |x_2| + \ldots + |x_n| = \sum_{i=1}^{n}{|x_i|}\\
    \|x\|_{\infty} &= \max \left(|x_1|, |x_2|, \ldots, |x_n| \right)
                      = \underset{i}{\max} \left( |x_i| \right)
\end{align*}
Matrices:
\begin{align*}
  \|A\| &= \max_{x\ne 0} \frac{\|Ax\|}{\|x\|}\\
  \|A\|_p &= \max_{x\ne 0} \frac{\|Ax\|_p}{\|x\|_p}\\
  \|A\|_1 &= \max_{1\leq j \leq n} \sum_{i=1}^{m}|a_{ij}|\\
  \|A\|_{\infty} &= \max_{1\leq i \leq m} \sum_{j=1}^{n}|a_{ij}|
\end{align*}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Some Important Properties of Norms}
\begin{align*}
     \|\alpha x\| &= |\alpha| \|x\|\\
     \|Ax\| &\le \|A\| \|x\| \\
     \|x+y\| &\le \|x\| + \| y \|
\end{align*}
\begin{alertblock}{}
Challenge: Make sure that you can prove these properties.
\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $b$}

Perturb $b$ with $\delta b$ such that
\begin{equation*}
    \frac{\norm{\delta b}}{\norm{b}} \ll 1,
\end{equation*}
The perturbed system is
\begin{equation*}
    A (x + \delta x_b) = b + \delta b
\end{equation*}
The perturbations satisfy
\begin{equation*}
    A \delta x_b = \delta b
\end{equation*}


Analysis shows (see next two slides for proof) that
\begin{equation*}
    \frac{\norm{\delta x_b}}{\norm{x}} \le \norm{A}\norm{A^{-1}} \frac{\norm{\delta b}}{{\norm{b}}}
\end{equation*}

Thus, the effect of the perturbation is small
\emph{only if} $\norm{A}\norm{A^{-1}}$ is small.
\begin{equation*}
    \frac{\norm{\delta x_b}}{\norm{x}} \ll 1
    \ \ \ \ \text{only if}
    \ \ \ \ \norm{A}\norm{A^{-1}} \sim 1
\end{equation*}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $b$ (Proof)}


Let $x + \delta x_b$ be the \emph{exact} solution to the perturbed system
\begin{equation}   \label{eq:axdb}
    A (x + \delta x_b) = b + \delta b
\end{equation}
Expand
\begin{equation*}
    Ax + A \delta x_b = b + \delta b
\end{equation*}
Subtract $Ax$ from left side and $b$ from right side since $Ax=b$
\begin{equation*}
    A \delta x_b = \delta b
\end{equation*}
Left multiply by $A^{-1}$
\begin{equation}   \label{eq:dxb}
    \delta x_b = A^{-1} \delta b
\end{equation}


% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $b$ (Proof, p. 2)}

Take norm of equation~\eqref{eq:dxb}
\begin{equation*}
    \norm{\delta x_b} = \norm{A^{-1}\,\delta b}
\end{equation*}
Applying consistency requirement of matrix norms
\begin{equation}   \label{eq:normdxb}
    \norm{\delta x} \le \norm{A^{-1}}\norm{\delta b}
\end{equation}
Similarly, $Ax=b$ gives $\norm{b} = \norm{Ax}$, and
\begin{equation}   \label{eq:normb}
    \norm{b} \le \norm{A}\norm{x}
\end{equation}
Rearrangement of equation~\eqref{eq:normb} yields
\begin{equation}     \label{eq:normxi}
    \frac{1}{\norm{x}} \le \frac{\norm{A}}{\norm{b}}
\end{equation}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $b$ (Proof)}


Multiply Equation~\eqref{eq:normxi} by Equation~\eqref{eq:normdxb} to get
\begin{equation}    \label{eq:dxbonx}
    \frac{\norm{\delta x_b}}{\norm{x}} \le \norm{A}\norm{A^{-1}} \frac{\norm{\delta b}}{{\norm{b}}}
\end{equation}

\begin{center}
\begin{minipage}{0.75\textwidth}
\textbf{Summary:}\par\vspace{1ex}
If $x + \delta x_b$ is the \emph{exact} solution to the perturbed system
\begin{equation*}
    A (x + \delta x_b) = b + \delta b
\end{equation*}
then
\begin{equation*}
    \frac{\norm{\delta x_b}}{\norm{x}} \le \norm{A}\norm{A^{-1}} \frac{\norm{\delta b}}{{\norm{b}}}
\end{equation*}
\end{minipage}
\end{center}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to $A$}

Perturb $A$ with $\delta A$ such that
\begin{equation*}
    \frac{\norm{\delta A}}{\norm{A}} \ll 1,
\end{equation*}
The perturbed system is
\begin{equation*}
    (A + \delta A) (x + \delta x_A) = b
\end{equation*}

Analysis shows that
\begin{equation*}
    \frac{\norm{\delta x_A}}{\norm{x + \delta x_A}} \le \norm{A}\norm{A^{-1}} \frac{\norm{\delta A}}{\norm{A}}
\end{equation*}

Thus, the effect of the perturbation is small
\emph{only if} $\norm{A}\norm{A^{-1}}$ is small.
\begin{equation*}
    \frac{\norm{\delta x_A}}{\norm{x + \delta x_A}} \ll 1
    \quad \text{only if}
    \quad \norm{A}\norm{A^{-1}} \sim 1
\end{equation*}


% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Effect of Perturbations to both $A$ and $b$}

Perturb both $A$ with $\delta A$ and $b$ with $\delta b$ such that
\begin{equation*}
    \frac{\norm{\delta A}}{\norm{A}} \ll 1
    \quad\text{and}\quad
    \frac{\norm{\delta b}}{\norm{b}} \ll 1
\end{equation*}
The perturbation satisfies
\begin{equation*}
    (A + \delta A) (x + \delta x) = b + \delta b
\end{equation*}
Analysis shows that
\begin{equation*}
    \frac{\norm{\delta x}}{\norm{x + \delta x}}
    \ \ \le \ \ \frac{\norm{A}\norm{A^{-1}}}{\ \ 1-\norm{A}\norm{A^{-1}} \frac{\norm{\delta A}}{\norm{A}} \ \ }
        \left[ \frac{\norm{\delta A}}{\norm{A}} + \frac{\norm{\delta b}}{\norm{b}}\right]
\end{equation*}
Thus, the effect of the perturbation is small
\emph{only if} $\norm{A}\norm{A^{-1}}$ is small.
\begin{equation*}
    \frac{\norm{\delta x}}{\norm{x + \delta x}} \ll 1
    \quad \text{only if} \quad
    \norm{A}\norm{A^{-1}} \sim 1
\end{equation*}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Condition number of $A$}

The \textbf{condition number}
\begin{equation*}
    \kappa(A) \equiv \norm{A}\norm{A^{-1}}
\end{equation*}
indicates the sensitivity of the solution to perturbations in $A$ and $b$. The
condition number can be measured with any $p$-norm.

The condition number is always in the range
\begin{equation*}
    1 \le \kappa(A) \le \infty
\end{equation*}
\begin{quote}
\begin{itemize}
    \item   $\kappa(A)$ is a mathematical property of $A$
    \item   \emph{Any} algorithm will produce a solution that is sensitive
            to perturbations in $A$ and $b$ if $\kappa(A)$ is large.
    \item   In exact math a matrix is either singular or non-singular.
            $\kappa(A)=\infty$ for a singular matrix
    \item   $\kappa(A)$ indicates how close
            $A$ is to being \emph{numerically} singular.
    \item   A matrix with large $\kappa$ is said to be \textbf{ill-conditioned}
\end{itemize}
\end{quote}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Computational Stability}

\textbf{In Practice}, applying Gaussian elimination with
partial pivoting and back substitution to $Ax=b$ gives the \textbf{exact
solution}, $\hat{x}$, to the \textbf{nearby problem}
\begin{equation*}
    (A + E)\hat{x} = b
    \ \ \ \
    \text{where}
    \ \ \ \
    \|E\|_{\infty} \le  \epsm \|A\|_{\infty}
\end{equation*}
\begin{quote}
    Gaussian elimination with partial pivoting and back substitution
    ``gives exactly the right answer to nearly the right question.''
    \par\hfill{--- Trefethen and Bau}
\end{quote}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Computational Stability}

An algorithm that gives the exact answer to a problem that is
near to the original problem is said to be \textbf{backward stable}.
Algorithms that are not backward stable will tend to amplify
roundoff errors present in the original data.  As a result, the
solution produced by an algorithm that is not backward stable will
not necessarily be the solution to a problem that is close to the
original problem.

Gaussian elimination without partial pivoting is \emph{not}
backward stable for arbitrary $A$.  

If $A$ is symmetric and positive
definite, then Gaussian elimination without pivoting is backward stable.


% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The Residual}

Let $\hat{x}$ be the numerical solution to $Ax=b$.
$\hat{x}\neq x$  ($x$ is the exact solution) because of roundoff.

The \textbf{residual} measures how close $\hat{x}$ is to satisfying
the original equation
\begin{equation*}
    r = b - A\hat{x}
\end{equation*}

It is not hard to show that
\begin{equation*}
    \frac{\norm{\hat{x} - x}}{\norm{x}}
       \ \le\ \kappa(A) \frac{\norm{r}}{\norm{b}}
\end{equation*}

Small \norm{r} does not guarantee a small \norm{\hat{x}-x}.

If $\kappa(A)$ is large the $\hat{x}$ returned by Gaussian
elimination and back substitution (or any other solution method) is not
guaranteed to be anywhere near the true solution to $Ax=b$.

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Rules of Thumb}

\begin{itemize}
    \item   Applying Gaussian elimination with partial pivoting and
            back substitution to $Ax=b$ yields a numerical solution
            $\hat{x}$ such that the residual vector $r=b-A\hat{x}$ is
            small \emph{even if} the $\kappa(A)$ is large.
    \item   If $A$ and $b$ are stored to machine precision $\epsm$, the
            numerical solution to $Ax=b$ by any (good) variant of Gaussian
            elimination is correct to $d$ decimal digits where
\begin{equation*}
                d = | \log_{10}(\epsm)| - \log_{10}\left(\kappa(A)\right)
\end{equation*}
\end{itemize}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Rules of Thumb}

\begin{equation*}
	d = | \log_{10}(\epsm)| - \log_{10}\left(\kappa(A)\right)
\end{equation*}

\textbf{Example:}\par
MATLAB\ computations have $\epsm\approx 2.2\times10^{-16}$.  For a system
with $\kappa(A)\sim10^{10}$ the elements of the solution vector will have
\begin{align*}
    d &= | \log_{10}(2.2\times10^{-16})| - \log_{10}\left(10^{10}\right) \\
      &\approx 16 - 10  \\
      &= 6
\end{align*}
correct digits

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Summary of Limits to Numerical Solution of $Ax=b$}

\begin{enumerate}
    \item   $\kappa(A)$ indicates how close $A$ is to being numerically singular
    \item   If $\kappa(A)$ is ``large'', $A$ is \textbf{ill-conditioned} and
            \emph{even the best} numerical algorithms will produce a solution,
            $\hat{x}$ that cannot be guaranteed to be close to the true
            solution, $x$
    \item   In practice, Gaussian elimination with partial pivoting and
            back substitution produces a solution with a small residual
\begin{equation*}
                r = b - A\hat{x}
\end{equation*}
            \emph{even if} $\kappa(A)$ is large.
\end{enumerate}

% ----------------------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{The Backslash Operator}

Consider the scalar equation
\begin{equation*}
    5x = 20
    \qquad
    \Longrightarrow
    \qquad
    x = (5)^{-1}20
\end{equation*}
% The value of $x$ is computed by multiplication of 20 by the inverse of 5.

The extension to a system of equations is, of course
\begin{equation*}
    Ax = b
    \qquad
    \Longrightarrow
    \qquad
    x = A^{-1}b
\end{equation*}
where $A^{-1}b$ is the formal solution to $Ax=b$

In MATLAB\ notation the system is solved with
\begin{lstlisting}[language=matlab]
    x = A\b
\end{lstlisting}


% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{The Backslash Operator}

Given an \matdim{n}{n} matrix $A$, and an \matdim{n}{1} vector $b$
the \verb|\| operator performs a sequence of tests on the $A$ matrix.
MATLAB\ attempts to solve the system with the method that gives the
least roundoff and the fewest operations.

When $A$ is an \matdim{n}{n} matrix:
\vspace{0.0cm}
\begin{enumerate}
    \item   MATLAB\ examines $A$ to see if it is a permutation of a triangular system
            \begin{itemize}
                \item[]   If so, the appropriate triangular solve is used.
            \end{itemize}
            \vspace{3ex}

    \item   MATLAB\ examines $A$ to see if it \emph{appears} to be symmetric and positive
            definite.
           \begin{itemize}
                \item[]   If so, MATLAB\ attempts a Cholesky factorization\newline
                          and two triangular solves.
           \end{itemize}
            \vspace{3ex}

    \item   If the Cholesky factorization fails, or if $A$ does not appear to
            be symmetric,
            \begin{itemize}
           \item[] MATLAB\ attempts an $LU$ factorization\newline
                    and two triangular solves.
        \end{itemize}
\end{enumerate}

% ----------------------
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{More Algorithms for Special Systems}
  \begin{itemize}
    \item tridiagonal systems
    \item banded systems
    \item LU decomposition
    \item Cholesky factorization
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Tridiagonal}
A tridiagonal matrix $A$
\begin{equation*}
  \begin{bmatrix}
d_1 & c_1 &       &         &       &       &          &    \\
a_1 & d_2 & c_2   &         &       &       &          &    \\
    & a_2 & d_3   & c_3     &       &       &          &    \\
    &     & \dots & \dots   & \dots &       &          &    \\
    &     &       & a_{i-1} & d_i   & c_{i} &          &    \\
    &     &       &         & \dots & \dots & \dots    &    \\
    &     &       &         & \dots & \dots & \dots    &    \\
    &     &       &         &       &       & a_{n-1}  & d_n   \\
  \end{bmatrix}
\end{equation*}
\begin{itemize}
  \item storage is saved by not saving zeros
  \item only $n+2(n-1) = 3n-2$ places are needed to store the matrix
versus $n^2$ for the whole system
  \item can operations be saved?  yes!
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Tridiagonal}
\begin{equation*}
  \begin{bmatrix}
d_1 & c_1 &       &         &       &       &          &    \\
a_1 & d_2 & c_2   &         &       &       &          &    \\
    & a_2 & d_3   & c_3     &       &       &          &    \\
    &     & \dots & \dots   & \dots &       &          &    \\
    &     &       & a_{i-1} & d_i   & c_{i} &          &    \\
    &     &       &         & \dots & \dots & \dots    &    \\
    &     &       &         & \dots & \dots & \dots    &    \\
    &     &       &         &       &       & a_{n-1}  & d_n   \\
  \end{bmatrix}
\end{equation*}
Start forward elimination (without any special pivoting)
\begin{enumerate}
  \item subtract $a_1/d_1$ times row 1 from row 2
  \item this eliminates $a_1$, changes $d_2$ and does not touch $c_2$
  \item continuing:
\begin{equation*}
  d_{i} = d_{i} - \left(\frac{a_{i-1}}{d_{i-1}} c_{i-1}\right)
\end{equation*}
for $i=2\dots n$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Tridiagonal}
\begin{equation*}
  \begin{bmatrix}
{\tilde{d}}_1 & c_1 &       &         &       &       &          &    \\
 & {\tilde{d}}_2 & c_2   &         &       &       &          &    \\
    & & {\tilde{d}}_3   & c_3     &       &       &          &    \\
    &     &  & \dots   & \dots &       &          &    \\
    &     &       &  & {\tilde{d}}_i   & c_{i} &          &    \\
    &     &       &         & & \dots & \dots    &    \\
    &     &       &         & & \dots & \dots    &    \\
    &     &       &         &       &       & & {\tilde{d}}_n   \\
  \end{bmatrix}
\end{equation*}
This leaves an upper triangular (2-band).  With back substitution:
\begin{enumerate}
  \item $x_n = {\tilde{b}}_n/{\tilde{d}}_n$
  \item $x_{n-1} = (1/{\tilde{d}}_{n-1})({\tilde{b}}_{n-1} - c_{n-1}x_n)$
  \item $x_{i} = (1/{\tilde{d}}_{i})({\tilde{b}}_{i} - c_{i}x_{i+1})$
\end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Tridiagonal Algorithm}
\begin{lstlisting}[mathescape]
  input: $n, a, d, c, b$
  for $i=2$ to $n$
    $xmult = a_{i-1}/d_{i-1}$
    $d_i = d_i - xmult \cdot c_{i-1}$
    $b_i = b_i - xmult \cdot b_{i-1}$
  end
  $x_{n} = b_{n}/d_{n}$
  for $i=n-1$ down to $1$
    $x_{i} = (b_{i} - c_{i} x_{i+1})/d_{i}$
  end
\end{lstlisting}
\begin{alertblock}{}
Challenge: Will this algorithm make good use of the processor cores in
a multicore processor?  Why or why not? 
\end{alertblock}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{$m$-band}
\begin{tabular}{c c c}
\pgfimage[width=0.3\textwidth]{./figs/band5} & 
\pgfimage[width=0.3\textwidth]{./figs/band11} & 
\pgfimage[width=0.3\textwidth]{./figs/band11b}\\
$m=5$ & $m=11$ & $m=11$\\
\end{tabular}
\begin{itemize}
  \item the $m$ correspond to the total width of the non-zeros
  \item after a few passes of GE \emph{fill-in} with occur within the band
  \item so an empty band costs (about) the same an a non-empty band
  \item one fix: reordering (e.g. Cuthill-McKee)
  \item generally GE will cost $\mathcal{O}(m^2 n)$ for $m$-band systems
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
