% Copyright Luke Olson 2009--2014
% This work is licensed under the Creative Commons
% Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a
% copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.
%
\documentclass[10pt]{beamer}
%\documentclass[handout,10pt]{beamer}
%
\mode<presentation>
{
  \usetheme[secheader]{Boadilla}
  \usefonttheme[onlymath]{serif}
  \setbeamercovered{invisible}
  \usecolortheme{luke}
  %\setbeamercovered{transparent}
  %
}
\mode<handout>
{
  \usetheme[secheader]{Boadilla}
  \usefonttheme[onlymath]{serif}
  \setbeamercovered{invisible}
  \usecolortheme{luke2}
  %\setbeamercovered{transparent}
}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{pxfonts}
\usepackage{eulervm}
\usepackage{listings}
%\usepackage{pgfpages}
%\pgfpagesuselayout{2 on 1}[letterpaper]
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%
%
%
\newcommand{\ACK}{\framesubtitle{{\tiny{ack: Y. Saad}}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\vb}{{\bf{b}}}
\newcommand{\ve}{{\bf{e}}}
\newcommand{\vg}{{\bf{g}}}
\newcommand{\vp}{{\bf{p}}}
\newcommand{\vr}{{\bf{r}}}
\newcommand{\vu}{{\bf{u}}}
\newcommand{\vx}{{\bf{x}}}
\newcommand{\vz}{{\bf{z}}}
\newcommand{\vA}{{\bf{A}}}
\newcommand{\vU}{{\bf{U}}}
\newcommand{\mO}{{\mathcal{O}}}
\newcommand{\mF}{{\mathcal{F}}}
\definecolor{mygray}{rgb}{0.95,0.95,0.95}
\setcounter{MaxMatrixCols}{20}
\lstset{
        language=matlab,
        numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt,
        basicstyle=\color{black}\ttfamily\small,
        commentstyle=\color{green}\ttfamily,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        showstringspaces=false,
        backgroundcolor=\color{mygray},
        breaklines,
}
\newcommand{\norm}[1]{{\ensuremath{{\|#1\|}}}}
\newcommand{\matdim}[2]{\ensuremath{#1\times#2}}
\newcommand{\rank}[1]{\ensuremath{\mathrm{rank}(#1)}}
\newcommand{\epsm}{\ensuremath{\varepsilon_m}}
\newcommand{\cmd}[1]{{\normalfont\ttfamily\bfseries#1}}

\author{L. Olson}
\institute[UIUC]
{Department of Computer Science\\
University of Illinois at Urbana-Champaign\\
\vspace{0.5cm}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pgfdeclareimage[height=0.5cm]{university-logo}{./figs/uiuclogo}
\logo{\pgfuseimage{university-logo}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[CS 357]{Lecture 10}
\subtitle{Sparse Matrices, Iterative Methods}
\date{September 24, 2009}

\begin{document}
% -------------------------------------------------
\begin{frame}
  \titlepage
\end{frame}
% -------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% -------------------------------------------------
\begin{frame}
\frametitle{An application}
Latent semantic analysis (LSA) analyzes two-mode data.  Looks at relationships
between documents and terms.
\begin{itemize}
    \item natural language processing
    \item information retrieval
    \item information filtering
    \item textual machine learning
\end{itemize}

Document-term matrix:
Document1(D1) = "I love numerical analysis"
Document1(D2) = "I do not love numerical analysis, but I love linear algebra."

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}\hline
   & I & love & numerical & linear & algebra\\\hline
D1 & 1 & 1    & 1         & 0      & 0 \\\hline
D2 & 1 & 2    & 0         & 1      & 1 \\\hline
\end{tabular}
\end{center}
\end{frame}
\begin{frame}
\frametitle{An application}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}\hline
   & I & love & numerical & linear & algebra\\\hline
D1 & 1 & 1    & 1         & 0      & 0 \\\hline
D2 & 1 & 2    & 0         & 1      & 1 \\\hline
\end{tabular}
\end{center}
\bigskip

One method for weights: Term Count Model

Variation: Term Frequency-Inverse Document Frequency; weight the entries inversely, highlighting infrequent terms
\bigskip

Let $X$ be the matrix of occurrences (or the inverse).
\begin{equation*}
X=
    \begin{bmatrix}
    x_{1,1} & \dots & x_{1,n}\\
    \vdots & \ddots & \vdots\\
    x_{m,1} & \dots & x_{m,n}\\
    \end{bmatrix}
\end{equation*}
Now each row will be a vector relating a term to all documents.  Each column
will be a vector relating a document to all terms.
\end{frame}
\begin{frame}
\frametitle{An application}
\begin{equation*}
X=
    \begin{bmatrix}
    x_{1,1} & \dots & x_{1,n}\\
    \vdots & \ddots & \vdots\\
    x_{m,1} & \dots & x_{m,n}\\
    \end{bmatrix}
\end{equation*}
\begin{itemize}
    \item $X$ has many zeros
    \item a dot product of the rows gives the correlation between terms over the
documents
    \item $XX^T$ gives a cumulative view of the correlation
    \item same with $X^T X$
    \item singular value decompositions, eigenvalue analysis, etc give other
information
\end{itemize}
    
\end{frame}

\begin{frame}
\frametitle{Sparse Matrices}
\ACK
\begin{center}
  \pgfimage[height=3cm]{./figs/sparse1}
\end{center}
  \begin{itemize}
  \item Vague defnition: matrix with few nonzero entries
  \item For all practical purposes: an $m\times n$ matrix is sparse if
it has $\mO(\min{(m, n)})$ nonzero entries.
  \item This means roughly a constant number of nonzero entries per row and column 
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Sparse Matrices}
\ACK
  \begin{itemize}
\item Other definitions use a slow growth of nonzero entries
with respect to $n$ or $m$.

\item Wilkinson's Definition: ``..matrices that allow special
techniques to take advantage of the large number of zero
elements." (J. Wilkinson)''

\item A few applications which lead to sparse matrices:
Structural Engineering, Computational Fluid Dynamics, Reservoir simulation, 
Electrical Networks, optimization, data analysis, 
information retrieval (LSI), circuit similation,
device simulation, $\ldots$
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Sparse Matrices: The Goal}
  \begin{itemize}
\item To perform standard matrix computations economically
i.e., without storing the zeros of the matrix.
\item For typical Finite Element /Finite difference matrices,
number of nonzero elements is $\mO(n)$.
  \end{itemize}
\begin{example}
To add two square dense matrices of size $n$
requires $\mO(n^2)$ operations. To add two sparse matrices $A$
and $B$ requires $\mO(nnz(A)) + nnz(B))$ where $nnz(X) =$
number of nonzero elements of a matrix $X$.
\end{example}

\begin{block}{remark}
$A^{-1}$ is usually dense, but $L$ and $U$ in the $LU$
factorization may be reasonably sparse (if a good technique
is used).
\end{block}
\end{frame}
% -------------------------------------------------
\begin{frame}
\frametitle{Goal}
  \begin{itemize}
    \item Principle goal: \emph{solve}
      \begin{equation*}
        A x= b
      \end{equation*}
      where $A \in \R^{n\times n}$, $x,b\in \R^{n}$
    \item Assumption: $A$ is very sparse
    \item General approach: iteratively improve the solution
    \item Given $x_0$, ultimate ``correction'' is
    \begin{equation*}
      x_1 = x_0 + e_0
    \end{equation*}
      where $e_0 = x-x_0$, thus $Ae_0=Ax-Ax_0$, 
    \item  or
    \begin{equation*}
      x_1 = x_0 + A^{-1} r_0
    \end{equation*}
      where $r_0 = b-Ax_0$
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Goal}
  \begin{itemize}
    \item Principle difficulty: how do we ``approximate'' $A^{-1}r$ or
reformulate the iteration?
    \item One simple idea:
    \begin{equation*}
      x_1 = x_0 + \alpha r_0
    \end{equation*}
    \item operation is inexpensive if $r_0$ is inexpensive
    \item requires very fast sparse mult-vec $Ax_0$
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Sparse Matrices}
  \begin{itemize}
    \item So how do we store $A$?
    \item Fast mult-vec is certainly important; also ask
      \begin{itemize}
        \item what type of access (rows, cols, diag, etc)?
        \item dynamic allocation?
        \item transpose needed?
        \item inherent structure?
      \end{itemize}
    \item unlike dense methods, not a lot of standards for iterative
      \begin{itemize}
        \item dense BLAS have been long accepted
        \item sparse BLAS still iterating
      \end{itemize}
    \item Even data structures for dense storage not as obvious
    \item sparse operations have low operation/memory reference ratio
  \end{itemize}
\end{frame}
% -------------------------------------------------
% % -------------------------------------------------
% \begin{frame}
% \frametitle{Sparse Matrix Qualification}
% \bigskip

% Some popular techniques:
%   \begin{itemize}
%     \item (COO) $Aij$:  allocate three equal length arrays.  One for $a_{ij}$,
% the other two for the indices $i,j$.  No order.
%     \item (CSR/CSC) compressed row (column): the data is stored row (column)-wise.
% Each row (column) is a contiguous block.  Column (row) entry is reduced
% storage.
%     \item (DIA/CDS) compressed diagonal: contiguous storage for diagonals.  allows
% fast diag access.  Many patterned options here (e.g. banded).
%   \end{itemize}

% \bigskip
% \tiny{ see ``Numerical Linear Algebra Algorithms and Software'' by
% Dongarra, Eijkhout}
% \end{frame}
% % -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Sparse Matrix Qualification}
Matrix Market attempts to classify the sparse matrix.

First Qualification (type of values and number of values):
\begin{center}
  \begin{tabular}{l l}\hline
  identifier & description\\\hline
  Real & All entries are float\\
  Complex & All entries are a pair of float\\
  Integer & All entries are int\\
  Pattern & Matrix is a pattern.  Actual entries are omitted\\
  \emph{Parallel} & \emph{Parallel structure is identified}\\\hline
  \end{tabular}
\end{center}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Sparse Matrix Qualification}
Second Qualification (interpreting values):
\begin{center}
  \begin{tabular}{l l}\hline
  identifier & description\\\hline
  General & $A$ has no symmetry, no symmetry is utilized, \\
          & or $A$ is not square\\
  Symmetric & $a_{ij}=a_{ji}$; only entries on the diagonal \\
            & and below(or above) are stored.\\
  Skew-Symmetric & $a_{ij}=-a_{ji}$; only etires below (or above) \\
                  & the diagonal ($=0$) are stored.\\
  Hermitian & $a_{ij}=\bar{a}_{ji}$; only entries on the diagonal \\
                & and below (or above) are stored.\\
  \end{tabular}
\end{center}

\bigskip
\tiny{ see ``The Matrix Market Exchange Formats: Initial Design'' by
Boisvert, Pozo, Remington}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Popular Storage Structures}
\begin{center}
\begin{tabular}{l l l l}
  {\bf DNS} & Dense                    & {\bf ELL} & Ellpack-Itpack\\
  {\bf BND} & Linpack Banded           & {\bf DIA} & Diagonal\\
  {\bf COO} & Coordinate               & {\bf BSR} & Block Sparse Row\\
  {\bf CSR} & Compressed Sparse Row    & {\bf SSK} & Symmetric Skyline\\
  {\bf CSC} & Compressed Sparse Column & {\bf BSR} & Nonsymmetric Skyline\\
  {\bf MSR} & Modified CSR             & {\bf JAD} & Jagged Diagonal\\
  {\bf LIL} & Linked List              & \\
\end{tabular}
\end{center}
\bigskip
note: CSR = CRS, CCS = CSC, SSK = SKS in some references

\begin{block}{Matlab :: CSC}
     John R. Gilbert, Cleve, Moler and Robert Schreiber,
 “Sparse Matrices in MATLAB: Design and Implementation”,
 SIAM Journal on Matrix Analysis and Applications, volume
 13, number 1, pages 333–356 (1992).
\end{block}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{DNS}
\begin{equation*}
A=
\begin{bmatrix}
1.0 & 2.0 & 3.0\\
4.0 & 5.0 & 6.0\\
7.0 & 8.0 & 9.0
\end{bmatrix}
\end{equation*}
\begin{equation*}
  AA = 
\begin{bmatrix}
3 & 3 & 1.0 & 2.0 & 3.0 & 4.0 & 5.0 & 6.0 & 7.0 & 8.0 & 9.0 \\
\end{bmatrix}
\end{equation*}
  \begin{itemize}
    \item simple
    \item row-wise
    \item easy blocked formats
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{COO}
\begin{equation*}
A=
\begin{bmatrix}
1 & 0 & 0 & 2 & 0\\
3 & 4 & 0 & 5 & 0\\
6 & 0 & 7 & 8 & 9\\
0 & 0 & 10 & 11 & 0\\
0 & 0 & 0 & 0 & 12
\end{bmatrix}
\end{equation*}
\begin{equation*}
\begin{array}{llllllllllllllll}
  AA &= & [ & 12.0 & 9.0 & 7.0 & 5.0 & 1.0 & 2.0 & 11.0 & 3.0 & 6.0 & 4.0 & 8.0 & 10.0 & ]\\
  JR &= & [ & 5    & 3   & 3   & 2   & 1   & 1   & 4    & 2   & 3   & 2   & 3   & 4    & ]\\
  JC &= & [ & 5    & 5   & 3   & 4   & 1   & 4   & 4    & 1   & 1   & 2   & 4   & 3    & ]\\
\end{array}
\end{equation*}
\begin{itemize}
  \item simple, often used for entry
\end{itemize}
\begin{alertblock}{}
Question: Do you need this much storage?
\end{alertblock}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{CSR}
\begin{equation*}
A=
\begin{bmatrix}
1 & 0 & 0 & 2 & 0\\
3 & 4 & 0 & 5 & 0\\
6 & 0 & 7 & 8 & 9\\
0 & 0 & 10 & 11 & 0\\
0 & 0 & 0 & 0 & 12\\
\end{bmatrix}
\end{equation*}
\begin{equation*}
\begin{array}{llllllllllllllll}
  AA &= & [ & 1.0 & 2.0 & 3.0 & 4.0 & 5.0 & 6.0 & 7.0 & 8.0 & 9.0 & 10.0 & 11.0 & 12.0 & ] \\
  JA &= & [ &  1 &4 &1 &2 &4 &1 &3 &4 &5 &3 &4 &5 & ] \\
  IA &= & [ &  1 &3 &6 &10 &12 &13 & ]&  &  &  &  & \\
\end{array}
\end{equation*}
  \begin{itemize}
    \item Length fo $AA$ and $JA$ is $nnz$; length of $IA$ is $n+1$
    \item $IA(j)$ gives the index (offset) to the beginning of row $j$
      in $AA$ and $JA$ (one origin due to Fortran)
    \item no structure, fast row access, slow column access (why?)
    \item related: CSC, MSR
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{MSR}
\begin{equation*}
A=
\begin{bmatrix}
1 & 0 & 0 & 2 & 0\\
3 & 4 & 0 & 5 & 0\\
6 & 0 & 7 & 8 & 9\\
0 & 0 & 10 & 11 & 0\\
0 & 0 & 0 & 0 & 12\\
\end{bmatrix}
\end{equation*}
\begin{equation*}
\begin{array}{llllllllllllll}
  AA = & [ 1.0 & 4.0 & 7.0 & 11.0 & 12.0 & *  & 2.0 & 3.0 & 5.0 & 6.0 & 8.0 & 9.0 & 10.0 ]\\
  JA = & [   7 & 8   & 10  & 13   & 14   & 14 & 4   & 1   & 4   & 1   & 4   & 5   & 3 ]\\
\end{array}
\end{equation*}
  \begin{itemize}
    \item places importance on diagonal (often nonzero and accessed
frequently)
    \item first $n$ entries are the diag
    \item $n+1$ is empty
    \item rest of $AA$ are the nondiagonal entries
    \item first $n+1$ entries in $JA$ give the index (offset) of the beginning
      of the row (the $IA$ of CSR is in this $JA$)
    \item rest of $JA$ are the columns indices
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{DIA}
\framesubtitle{or CDS}
\begin{equation*}
A=
\begin{bmatrix}
1 & 0 & 2 & 0 & 0\\
3 & 4 & 0 & 5 & 0\\
0 & 6 & 7 & 0 & 8\\
0 & 0 & 9 & 10 & 0\\
0 & 0 & 0 & 11 & 12\\
\end{bmatrix}
\quad
DIAG =
\begin{bmatrix}
  * & 1.0 & 2.0\\
3.0 & 4.0 & 5.0\\
6.0 & 7.0 & 8.0\\
9.0 & 10.0 & * \\
11.0 & 12.0 & *\\
\end{bmatrix}
\quad
IOFF=
\begin{bmatrix}
  -1 & 0 & 2\\
\end{bmatrix}
\end{equation*}
  \begin{itemize}
    \item need to know the offset structure
    \item some entries will always be empty
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{ELL}
\begin{equation*}
A=
\begin{bmatrix}
1 & 0 & 2 & 0 & 0\\
3 & 4 & 0 & 5 & 0\\
0 & 6 & 7 & 0 & 8\\
0 & 0 & 9 & 10 & 0\\
0 & 0 & 0 & 11 & 12\\
\end{bmatrix}
\quad
COEF =
\begin{bmatrix}
1.0 & 2.0 & 0.0\\
3.0 & 4.0 & 5.0\\
6.0 & 7.0 & 8.0\\
9.0 & 10.0 & 0.0 \\
11.0 & 12.0 & 0.0\\
\end{bmatrix}
\quad
JCOEF=
\begin{bmatrix}
  1 & 3 & 1\\
  1 & 2 & 4\\
  2 & 3 & 5\\
  3 & 4 & 4\\
  4 & 5 & 5\\
\end{bmatrix}
\end{equation*}
  \begin{itemize}
    \item Form columns from first non-zero in each row, repeat.
    \item used more on vector machines (what? why?)
    \item assumes low number of $nnz$ per row (=number of columns in
$COEFF$ and $JCOEFF$)
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{JDS}
  \begin{itemize}
    \item like DIA (and CDS),but more space-efficient
    \item costs more to gather and scatter
  \end{itemize}
Take
\begin{equation*}
  A=
\begin{bmatrix}
  10 & -3 & 0 & 1 & 0 & 0\\
0 & 9 & 6 & 0 & -2 & 0\\
3 & 0 & 8 & 7 & 0 & 0\\
0 & 6 & 0 & 7 & 5 & 4\\
0 & 0 & 0 & 0 & 9 & 13\\
0 & 0 & 0 & 0 & 5 & -1\\
\end{bmatrix}
\end{equation*}
And shift:
\begin{equation*}
  A \rightarrow
\begin{bmatrix}
  10 & -3 & 1 & \\
  9 & 6 & -2 & \\
3 & 8 & 7 & \\
6 & 7 & 5 & 4\\
9 & 13 & &\\
5 & -1 & &\\
\end{bmatrix}
\end{equation*}
\end{frame}
% -------------------------------------------------
\begin{frame}
\begin{equation*}
  A \rightarrow
\begin{bmatrix}
  10 & -3 & 1 & \\
  9 & 6 & -2 & \\
3 & 8 & 7 & \\
6 & 7 & 5 & 4\\
9 & 13 & &\\
5 & -1 & &\\
\end{bmatrix}
\end{equation*}
Now store the columns and column indices:
\begin{equation*}
VAL=
\begin{bmatrix}
10 & 9 & 3 & 6 & 9 & 5 \\
-3 & 6 & 8 & 7 & 13 & -1\\
1 & -2 & 7 & 5 & 0 & 0\\
0 & 0 & 0 & 4 & 0 & 0\\
\end{bmatrix}
\quad
COL=
\begin{bmatrix}
1 & 2 & 1 & 2 & 5 & 5\\
2 & 3 & 3 & 4 & 6 & 6\\
4 & 5 & 4 & 5 & 0 & 0\\
0 & 0 & 0 & 6 & 0 & 0\\
\end{bmatrix}
\end{equation*}

\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{JDS}
  \begin{itemize}
    \item now reorder in terms of largest to smallest $nnz$ in each row
    \item in JDS, the number of jagged diagonals is the number of $nnz$
in the first row of $VAL$.  This is the max $nnz$ in any row.
    \item $PERM$: permutation array to reorder rows
    \item $JDIAG$: jagged diags in order
    \item $COL$: column indices
    \item $JDPTR$: points to the begining of each diagonal
    \item advantage for mat-mat (see ``Krylov subspace methods on
supercomputers'' by Saad)
    \item this is actually ITPACK or Purdue storage
  \end{itemize}
\begin{equation*}
\begin{array}{llllllllllllllllll}
JDIAG = &[ 6 & 9 & 3 & 10 & 9 & 5; & 7 & 6 & 8 & -3 & 13 & -1; & 5 & -2 & 7 & 1; & 4;]\\
COL =   &[ 2 & 2 & 1 & 1 & 5 & 5; & 4 &3 & 3 & 2 & 6 & 6; & 5 & 5 &4 & 4; & 6 ]\\
PERM  = &[ 4 & 2 &3 &1 &5 &6] & & & & & & & & & & &\\
JDPTR  = &[ 1 & 7 &13 & 17] & & & & & & & & & & & & &\\
\end{array}
\end{equation*}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Blocked}
\begin{equation*}
A=
\begin{bmatrix}
1.0 &  2.0 &  0.0 &  0.0 &  3.0 &  4.0 \\
5.0 &  6.0 &  0.0 &  0.0 &  7.0 &  8.0 \\
0.0 &   0.0 &  9.0 & 10.0 & 11.0 & 12.0 \\
0.0 &   0.0 & 13.0 & 14.0 & 15.0 & 16.0 \\
17.0 & 18.0 & 0.0 &  0.0 &  20.0 & 21.0 \\
22.0 & 23.0 & 0.0 &   0.0 & 24.0 & 25.0 \\
\end{bmatrix}
\end{equation*}
\begin{equation*}
  AA = 
\begin{bmatrix}
  1.0 & 3.0 & 9.0 & 11.0 & 17.0 & 20.0\\
  5.0 & 7.0 & 15.0 & 13.0 & 22.0 & 24.0\\
  2.0 & 4.0 & 10.0 & 12.0 & 18.0 & 21.0\\
 6.0 & 8.0 & 14.0 & 16.0 & 23.0 & 25.0\\
\end{bmatrix}
\end{equation*}
\begin{align*}
  JA &= 
\begin{bmatrix}
  1 & 5 & 3 & 5 & 5 & 1 & 5\\
\end{bmatrix}\\
  IA &= 
\begin{bmatrix}
  1 & 3 & 5 & 7\\
\end{bmatrix}
\end{align*}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Blocked}
  \begin{itemize}
    \item each column of $AA$ is a $2\times 2$ block
    \item $JA(k) = $ column index of $(1,1)$ entries of the $kth$ block
    \item declared as $AA(2,2,6)$
    \item blocks arise in \emph{many} apps
    \item variant: variable block size
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Blocked}
Also row-wise
\begin{equation*}
  AA =
\begin{bmatrix}
  1.0 & 5.0 & 2.0 & 6.0\\
  3.0 & 7.0 & 4.0 & 8.0\\
  9.0 & 15.0 & 10.0 & 14.0\\
  11.0 & 13.0 & 12.0 & 16.0\\
  17.0 & 22.0 & 18.0 & 23.0\\
  20.0 & 24.0 & 21.0 & 25.0\\
\end{bmatrix}
\end{equation*}
\begin{align*}
  JA &= 
\begin{bmatrix}
  1 & 5 & 3 & 5 & 5 & 1 & 5\\
\end{bmatrix}\\
  IA &= 
\begin{bmatrix}
  1 & 3 & 5 & 7\\
\end{bmatrix}
\end{align*}
  \begin{itemize}
    \item each row of $AA$ is a $2\times 2$ block (can be a drawback)
    \item $JA$, $IA$ same, $AA(6,2,2)$
    \item if elements of blocks are accessed at the same time: rows
are better (C)
    \item if elements of similar positions in different blocks are
accessed at the same time: columns are better (C)
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{SSK, NSK}
  \begin{itemize}
  \item for ``skyline'' matrices (variable band, {\tiny{see ``Direct
methods for sparse matrices'' by Duff, Erisman, Reid}})
  \item can be used for diagonal block matrices
  \item skyline structure is preserved in basic GE
  \item for symmetric: Place all the rows (in order) into $VAL$
  \item $IA$ points to the beginning of each row
  \item $JA$ implicit
  \item for nonsymmetric: store $L$ in SSK format, then $U$ in
column-wise $SSK$  {\tiny see ``SPARSEKIT'' by Saad}
  \end{itemize}
\begin{center}
  \pgfimage[height=4cm]{./figs/sks}
\end{center}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{LIL}
  \begin{itemize}
  \item Similar to CSR, but rather than a flat $AA$ vector, each row is
a linked list of elements
  \item first element of each row is accessed by $ROOT$
  \item each element in $AA$ has a corresponding $NEXT$ entry
  \item $-1$ indicates the end of a row
  \item column lookup take $\mO(nnz)$; one semi-costly fix: store a
columnwise index in the same way as rows.
  \item very good element insertion time, but more memory
  \end{itemize}
\end{frame}
% -------------------------------------------------
\begin{frame}
\frametitle{tri it...}
    \begin{equation*}
  A = \begin{bmatrix}
  7 & 0 &0 &0 &0 &0\\
  0&1&2 &0&0&0\\
  0&2&0&2&0&0\\
  0&0&0&0&5&0\\
  0&0&0&0&6&4\\
  \end{bmatrix}
\end{equation*}
\begin{itemize} 
    \item CSR
    \item CSC
    \item COO
    \item LIL
\end{itemize} 
\end{frame}
% -------------------------------------------------
\begin{frame}[shrink]
\frametitle{Example}
\vspace{-0.5cm}

\begin{columns}
  \begin{column}{0.48\textwidth}
    \begin{equation*}
  A = \begin{bmatrix}
  7 & 0 &0 &0 &0 &0\\
  0&1&2 &0&0&0\\
  0&2&0&2&0&0\\
  0&0&0&0&5&0\\
  0&0&0&0&6&4\\
  \end{bmatrix}
\end{equation*}
  \end{column}
  \begin{column}{0.48\textwidth}
    \begin{center}
    \begin{tabular}{c | c c c}
    $i$ & $IA$ & $JA$ & $AA$\\\hline
    1 & 2 & 2 & 1\\
    2 & 3 & 4 & 2\\
    3 & 4 & 5 & 5\\
    4 & 2 & 3 & 2\\
    5 & 5 & 6 & 4\\
    6 & 1 & 1 & 7\\
    7 & 5 & 5 & 6\\
    8 & 3 & 2 & 2\\
    \end{tabular}
    COO
    \end{center}
  \end{column}
\end{columns}
\begin{columns}
  \begin{column}{0.48\textwidth}
    \begin{center}
    \begin{tabular}{c | c c c}
    $i$ & $IA$ & $JA$ & $AA$\\\hline
    1 & 1 & 1 & 7\\
    2 & 2 & 2 & 1\\
    3 & 4 & 3 & 2\\
    4 & 6 & 2 & 2\\
    5 & 7 & 4 & 2\\
    6 & 9 & 5 & 5\\
    7 & - & 5 & 6\\
    8 & - & 6 & 4\\
    \end{tabular}
    CSR
    \end{center}
  \end{column}
  \begin{column}{0.48\textwidth}
    \begin{center}
    \begin{tabular}{c | c c c c}
    $i$ & $IA$ & $JA$ & $AA$ & $NEXT$\\\hline
    1 & 4 & 3 & 2 & -1\\
    2 & 3 & 2 & 2 &  5\\
    3 & 2 & 2 & 1 &  1\\
    4 & 8 & 1 & 7 & -1\\
    5 & 7 & 4 & 2 & -1\\
    6 & - & 5 & 6 &  6\\
    7 & - & 6 & 4 & -1\\
    8 & - & 5 & 5 & -1\\
    \end{tabular}
    LIL
    \end{center}
  \end{column}
\end{columns}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Sparse Matrix-Vector}
$z = Ax$, $A_{m\times n}$, $x_{n\times 1}$, $z_{m\times 1}$
\begin{lstlisting}[mathescape]
input $A$, $x$
$z=0$
for $i=1$ to $m$
  for $col = A(i,:)$
    $z(i) += A(i,col)x(col)$
  end
end
\end{lstlisting}
  \begin{itemize}
  \item difference between CSR and LIL is computing line 4
  \item CSR: rows are contiguous...(next slide)
  \item LIL: follow rows through linked list
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Sparse Matrix-Vector}
\framesubtitle{CSR}
$z = Ax$, $A_{m\times n}$, $x_{n\times 1}$, $z_{m\times 1}$
\begin{lstlisting}[mathescape, language=FORTRAN]
DO I=1, m
  Z(I)=0
  K1 = IA(I)
  K2 = IA(I+1)-1
  DO J=K1, K2
    z(I) = z(I) + A(J)*x(JA(J))
  ENDDO
ENDDO
\end{lstlisting}
  \begin{itemize}
  \item $\mO(nnz)$
  \item marches down the rows
  \item very cheap
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Sparse Matrix-Vector}
\framesubtitle{CSR and Data Streams}
$z = Ax$, $A_{m\times n}$, $x_{n\times 1}$, $z_{m\times 1}$
\begin{lstlisting}[mathescape, language=FORTRAN]
DO I=1, M/2
  z(I)=0
  z(I+M/2)=0
  K1 = IA(I)
  K2 = IA(I+1)-1
  K3 = IA(I+M/2)
  K4 = IA(I+M/2+1)-1
  DO J=0,MIN(K2-K1,K4-K3)
    z(I) = z(I) + A(K1+J)*x(JA(K1+J))
    z(I+M/2) = z(I+M/2) + A(K3+J)*x(JA(K3+J))
  ENDDO
  ! ... finish up 
ENDDO
\end{lstlisting}
  \begin{itemize}
  \item $IA$ structure allows two data streams
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Sparse Matrix-Matrix}
  \begin{itemize}
  \item ways to optimize (``SMPP'', Douglas, Bank)
  \end{itemize}
$Z = AB$, $A_{m\times n}$, $B_{n\times p}$, $z_{m\times p}$
\begin{lstlisting}[mathescape]
for $i=1$ to $m$
  for $j=1$ to $n$
    $Z(i,j) = dot(A(i,:),B(:,j))$
  end
end
return $Z$
\end{lstlisting}
  \begin{itemize}
  \item obvious problem: column selection of $B$ is expensive for CSR
  \item not-so-obvious problem: $Z$ is sparse(!!), but the algorithm
doesn't account for this.
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Sparse Matrix-Matrix}
$Z = AB$, $A_{m\times n}$, $B_{n\times p}$, $z_{m\times p}$
\begin{lstlisting}[mathescape]
Z=0
for $i=1$ to $m$
  for $colA = A(i,:)$
    for $colB = A(colA,:)$
      $Z(i,colB) += A(i,colA)\cdot B(colA,colB)$
    end
  end
end
return $Z$
\end{lstlisting}
  \begin{itemize}
    \item only marches down rows
    \item only computes nonzero entries in $Z$ (aside from fortuitous
subtractions)
    \item line 5 will do and insert into $Z$.  Two options:
      \begin{enumerate}
        \item precompute sparsity of $Z$ in CSR
        \item use LIL for $Z$
      \end{enumerate}
  \end{itemize}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{}
\begin{center}
  \pgfimage[height=7cm]{./figs/matmat}
\end{center}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Some Matlab}
\begin{columns}
  \begin{column}{0.48\textwidth}
    \begin{equation*}
  A = \begin{bmatrix}
  7 & 0 &0 &0 &0 &0\\
  0&1&2 &0&0&0\\
  0&2&0&2&0&0\\
  0&0&0&0&5&0\\
  0&0&0&0&6&4\\
  \end{bmatrix}
\end{equation*}
  \end{column}
  \begin{column}{0.48\textwidth}
    \begin{center}
    \begin{tabular}{c | c c c}
    $i$ & $IA$ & $JA$ & $AA$\\\hline
    1 & 2 & 2 & 1\\
    2 & 3 & 4 & 2\\
    3 & 4 & 5 & 5\\
    4 & 2 & 3 & 2\\
    5 & 5 & 6 & 4\\
    6 & 1 & 1 & 7\\
    7 & 5 & 5 & 6\\
    8 & 3 & 2 & 2\\
    \end{tabular}
    COO
    \end{center}
  \end{column}
\end{columns}
\begin{lstlisting}
I=[2 3 4 2 5 1 5 3 ];
J=[2 4 5 3 6 1 5 2 ];
A=[1 2 5 2 4 7 6 2 ];

A=sparse(I,J,A);
\end{lstlisting}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}[fragile]
\frametitle{Some Matlab}
From COO to CSC:
\begin{lstlisting}
I=[2 3 4 2 5 1 5 3 ];
J=[2 4 5 3 6 1 5 2 ];
A=[1 2 5 2 4 7 6 2 ];

A=sparse(I,J,A);
\end{lstlisting}
\bigskip

Nonzeros and memory footprint:
\begin{lstlisting}
whos A
nnz(A)
\end{lstlisting}
\bigskip

To full and view:
\begin{lstlisting}
B=full(A)
spy(A)
\end{lstlisting}
\end{frame}
% -------------------------------------------------
% -------------------------------------------------
\begin{frame}
\frametitle{Simple Matrix Iterations}
  \begin{itemize}
    \item Solve
      \begin{equation*}
        A x= b
      \end{equation*}
%      where $A \in \R^{n\times n}$, $x,b\in \R^{n}$
    \item Assumption: $A$ is very sparse
    \item Let $A = N + M$, then
      \begin{eqnarray*}
        A x &=& b\\
        (N+M)x &=& b\\
        Nx &=& b - Mx
      \end{eqnarray*}
    \item Make this into an iteration:
      \begin{eqnarray*}
        Nx_k &=& b - M x_{k-1}\\
        x_k &=& N^{-1}(b - M x_{k-1})
      \end{eqnarray*}
    \item Careful choice of $N$ and $M$ can give effective methods
    \item More powerful iterative methods exist
    \end{itemize}
\end{frame}
% -------------------------------------------------
\end{document}
